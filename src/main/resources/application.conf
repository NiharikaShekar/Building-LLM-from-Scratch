# The cuurent environment is set here
environment = "local"

# Local environment configuration
local {
  fs.defaultFS = "hdfs://localhost:9000"
  mapreduce.job.reduces = 1
  mapreduce.input.fileinputformat.split.maxsize = 128000000 // 128 MB
  inputPath = "/input/input.txt"
  outputPath = "/output/"
  job {
    name = "TokenizerJobinLocal"
  }
  embedding {
    name = "EmbedderinLocal"
  }
}

# Test environment configuration
test {
  fs.defaultFS = "local"  # Running on local filesystem for test
  mapreduce.job.reduces = 1
  mapreduce.input.fileinputformat.split.maxsize = 128000000 // 128 MB
  inputPath = "/Users/niharikabelavadishekar/Documents/Cloud_Assignment/LLM_Assignment1/src/main/resources/input.txt"
  outputPath = "/Users/niharikabelavadishekar/Documents/Cloud_Assignment/LLM_Assignment1/src/main/resources/output/"
  job {
    name = "TokenizerTestJob"
  }
  embedding {
    name = "EmbedderTest"
  }
}

# Cloud environment configuration
cloud {
  fs.defaultFS = "s3://cs441-niharika"
  mapreduce.job.reduces = 3
  mapreduce.input.fileinputformat.split.maxsize = 256000000 // 256 MB
  inputPath = "s3://cs441-niharika/input/tamer.txt"
  outputPath = "s3://cs441-niharika/output/"
  job {
    name = "TokenizerCloudJob"
  }
  embedding {
    name = "EmbedderCloud"
  }
}

# Common config
common {
  vocabSize = 200000
  embeddingDim = 150
  numEpochs = 100
}
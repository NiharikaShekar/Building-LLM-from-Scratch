# Local environment configuration
local {
  fs.defaultFS = "hdfs://localhost:9000"
  mapreduce.job.reduces = 1
  mapreduce.input.fileinputformat.split.maxsize = 128000000 // 128 MB
  inputPath = "/input/input.txt"
  outputPath = "/output/"
  job {
    name = "TokenizerJobinLocal"
  }
  embedding {
    name = "EmbedderinLocal"
  }
}

# Test environment configuration
test {
  fs.defaultFS = "local"  # Running on local filesystem for test
  mapreduce.job.reduces = 1
  mapreduce.input.fileinputformat.split.maxsize = 128000000 // 128 MB
  inputPath = "/Users/nithish/project/test/input.txt"
  outputPath = "/Users/nithish/project/test/output/"
  job {
    name = "TokenizerTestJob"
  }
  embedding {
    name = "EmbedderTest"
  }
}

# Cloud environment configuration
cloud {
  fs.defaultFS = "s3://your-bucket"
  mapreduce.job.reduces = 3
  mapreduce.input.fileinputformat.split.maxsize = 256000000 // 256 MB
  inputPath = "s3://your-bucket/input/"
  outputPath = "s3://your-bucket/output/"
  job {
    name = "TokenizerCloudJob"
  }
  embedding {
    name = "EmbedderCloud"
  }
}

# Common config
common {
  vocabSize = 200000
  embeddingDim = 150
  numEpochs = 100
}